# æ–‡ä»¶è·¯å¾„: ai_services/rag/deployer.py
# æè¿°: [é‡æ„å] RAGéƒ¨ç½²å™¨æœåŠ¡ï¼Œå·²å®Œå…¨è§£è€¦ï¼Œé€šè¿‡ä¾èµ–æ³¨å…¥æ¨¡å¼è¿è¡Œã€‚
# ç‰ˆæœ¬: 2.0 (Decoupled & Reviewed)

import json
import logging
from collections import defaultdict
from pathlib import Path
from typing import Union

import vertexai
from vertexai import rag
from google.cloud import storage

# ä»åŒçº§ç›®å½•çš„ schemas.py å¯¼å…¥Pydanticæ¨¡å‹
from .schemas import NarrativeBlueprint, IdentifiedFact


class RagDeployer:
    """
    RAGéƒ¨ç½²å™¨æœåŠ¡ (RAG Deployer Service)ã€‚

    æœ¬æœåŠ¡è´Ÿè´£å°†èåˆäº†å¢å¼ºäº‹å®çš„å‰§æœ¬æ•°æ®ï¼Œå¤„ç†æˆRAGå¼•æ“æ‰€éœ€çš„å¯Œæ–‡æœ¬æ–‡æ¡£ï¼Œ
    ä¸Šä¼ è‡³Google Cloud Storage (GCS)ï¼Œå¹¶è§¦å‘Vertex AI RAGå¼•æ“çš„æ–‡ä»¶åŒæ­¥ã€‚

    è®¾è®¡åŸåˆ™:
    - **è§£è€¦**: ä¸ç›´æ¥ä¾èµ–ä»»ä½•æ¡†æ¶ï¼ˆå¦‚Djangoï¼‰ã€‚æ‰€æœ‰é…ç½®ï¼ˆé¡¹ç›®IDã€å¯†é’¥ã€è·¯å¾„ï¼‰å‡é€šè¿‡ä¾èµ–æ³¨å…¥ä¼ å…¥ã€‚
    - **èŒè´£å•ä¸€**: ä¸“æ³¨äºâ€œéƒ¨ç½²RAGè¯­æ–™åº“â€è¿™ä¸€æ ¸å¿ƒä»»åŠ¡ã€‚
    - **å¹‚ç­‰æ€§**: èƒ½å¤Ÿå¤„ç†è¯­æ–™åº“å·²å­˜åœ¨ï¼ˆæ›´æ–°ï¼‰å’Œä¸å­˜åœ¨ï¼ˆåˆ›å»ºï¼‰ä¸¤ç§æƒ…å†µã€‚
    """

    def __init__(self, project_id: str, location: str, logger: logging.Logger):
        """
        åˆå§‹åŒ–RAGéƒ¨ç½²å™¨ã€‚

        Args:
            project_id (str): Google Cloud é¡¹ç›®IDã€‚
            location (str): Google Cloud åŒºåŸŸ (e.g., "us-central1")ã€‚
            logger (logging.Logger): ä¸€ä¸ªç”±å¤–éƒ¨è°ƒç”¨æ–¹ä¼ å…¥çš„ã€å·²é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨å®ä¾‹ã€‚
        """
        self.project_id = project_id
        self.location = location
        self.logger = logger

        # åˆå§‹åŒ–Vertex AI SDKï¼Œåªéœ€è¦åœ¨æœåŠ¡å®ä¾‹åŒ–æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚
        try:
            vertexai.init(project=self.project_id, location=self.location)
            self.logger.info(f"RagDeployer initialized for project '{self.project_id}' in '{self.location}'")
        except Exception as e:
            self.logger.error(f"Vertex AI initialization failed: {e}", exc_info=True)
            raise

    def execute(self,
                corpus_display_name: str,
                blueprint_path: Path,
                facts_path: Path,
                gcs_bucket_name: str,
                staging_dir: Path,
                org_id: str,  # [ä¿®æ”¹] instance_id -> org_id
                asset_id: str):
        """
        æ‰§è¡Œå®Œæ•´çš„éƒ¨ç½²æµç¨‹ã€‚

        æ­¤æ–¹æ³•ç¼–æ’äº†ä»æ•°æ®èåˆåˆ°æœ€ç»ˆè§¦å‘RAGå¼•æ“åŒæ­¥çš„å…¨éƒ¨æ­¥éª¤ã€‚

        Args:
            corpus_display_name (str): RAGè¯­æ–™åº“çš„ç›®æ ‡æ˜¾ç¤ºåç§°ã€‚è¿™æ˜¯å®ç°ç§Ÿæˆ·éš”ç¦»çš„å…³é”®ï¼Œ
                                       é€šå¸¸ç”± "series_id" å’Œ "instance_id" æ‹¼æ¥è€Œæˆã€‚
            blueprint_path (Path): æœ¬åœ°ä¸´æ—¶ç›®å½•ä¸­ narrative_blueprint.json æ–‡ä»¶çš„è·¯å¾„ã€‚
            facts_path (Path): æœ¬åœ°ä¸´æ—¶ç›®å½•ä¸­ character_facts.json æ–‡ä»¶çš„è·¯å¾„ã€‚
            gcs_bucket_name (str): ç”¨äºæš‚å­˜RAGæºæ–‡ä»¶çš„GCSæ¡¶åç§°ã€‚
            staging_dir (Path): ç”¨äºåœ¨æœ¬åœ°ç”Ÿæˆå¯Œæ–‡æœ¬æ–‡æ¡£çš„ä¸´æ—¶ç›®å½•ã€‚
            instance_id (str): ç§Ÿæˆ·å®ä¾‹IDï¼Œç”¨äºåœ¨GCSä¸­åˆ›å»ºéš”ç¦»çš„æ–‡ä»¶å¤¹ç»“æ„ã€‚

        Returns:
            Dict: ä¸€ä¸ªåŒ…å«éƒ¨ç½²ç»“æœä¿¡æ¯çš„å­—å…¸ï¼Œç”¨äºCelery Taskè®°å½•ã€‚
        """
        self.logger.info("=" * 20 + f" ğŸš€ RAG éƒ¨ç½²ä»»åŠ¡å¯åŠ¨ (Corpus: {corpus_display_name}) ğŸš€ " + "=" * 20)

        try:
            # æ­¥éª¤ 1 & 2: åœ¨æœ¬åœ°èåˆæ•°æ®å¹¶ç”ŸæˆRAGæ‰€éœ€çš„å¯Œæ–‡æœ¬æ–‡ä»¶ï¼ŒåŒæ—¶æ„å»ºGCSçš„ç›®æ ‡URIã€‚
            gcs_uri, total_scenes = self._fuse_and_prepare_files(
                source_blueprint_path=blueprint_path,
                enhanced_facts_path=facts_path,
                staging_dir=staging_dir,
                gcs_bucket_name=gcs_bucket_name,
                org_id=org_id,
                asset_id=asset_id
            )

            # æ­¥éª¤ 3: å°†æœ¬åœ°ç”Ÿæˆçš„å¯Œæ–‡æœ¬æ–‡ä»¶æ‰¹é‡ä¸Šä¼ åˆ°GCSã€‚
            self._upload_dir_to_gcs(
                local_dir=staging_dir,
                gcs_uri=gcs_uri,
            )

            # æ­¥éª¤ 4: æŒ‡ç¤ºVertex AI RAGå¼•æ“ä»GCSæ‹‰å–å¹¶åŒæ­¥æ–‡ä»¶ã€‚
            self._deploy_to_rag_engine(
                corpus_display_name=corpus_display_name,
                gcs_uri=gcs_uri
            )

            self.logger.info("=" * 70)
            self.logger.info(f"âœ… ç§Ÿæˆ· '{org_id}' çš„RAGéƒ¨ç½²ä»»åŠ¡å·²æˆåŠŸå¯åŠ¨ï¼")
            self.logger.info(f"   ç›®æ ‡è¯­æ–™åº“: {corpus_display_name}")
            self.logger.info("   è¯·å‰å¾€Google Cloudæ§åˆ¶å°æŸ¥çœ‹æ–‡ä»¶å¯¼å…¥è¿›åº¦ã€‚")
            self.logger.info("=" * 70 + "\n")

            # è¿”å›å…³é”®ä¿¡æ¯ï¼Œä»¥ä¾¿Celery Taskå­˜å…¥æœ€ç»ˆçš„ä»»åŠ¡ç»“æœä¸­ã€‚
            return {
                "message": "RAG deployment process initiated successfully.",
                "corpus_name": corpus_display_name,
                "source_gcs_uri": gcs_uri,
                "total_scene_count": total_scenes
            }

        except Exception as e:
            self.logger.critical(f"éƒ¨ç½²æµç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©CeleryçŸ¥é“ä»»åŠ¡å¤±è´¥

    def _fuse_and_prepare_files(self, source_blueprint_path: Path, enhanced_facts_path: Path, staging_dir: Path,
                                gcs_bucket_name: str, org_id: str, asset_id: str) -> tuple[str, int]:
        """åœ¨æœ¬åœ°å¤„ç†æ–‡ä»¶ï¼šåŠ è½½ã€èåˆã€ç”Ÿæˆå¯Œæ–‡æœ¬ã€‚"""
        self.logger.info(f"â–¶ï¸ æ­¥éª¤ 1/4: æ­£åœ¨åŠ è½½ç§Ÿæˆ· '{org_id}' çš„æºæ•°æ®......")
        try:
            # ä½¿ç”¨Pydanticæ¨¡å‹åŠ è½½å’ŒéªŒè¯è¾“å…¥æ–‡ä»¶ï¼Œç¡®ä¿æ•°æ®ç»“æ„æ­£ç¡®ã€‚
            blueprint = NarrativeBlueprint.parse_file(source_blueprint_path)
            with enhanced_facts_path.open('r', encoding='utf-8') as f:
                facts_data = json.load(f)

            # å°†æ‰å¹³çš„factsåˆ—è¡¨è½¬æ¢ä¸ºPydanticå¯¹è±¡ï¼Œå¹¶æ³¨å…¥äº‹å®çš„å½’å±è€…ï¼ˆcharacter_nameï¼‰ã€‚
            all_facts = []
            facts_by_character_map = facts_data.get("identified_facts_by_character", {})
            for char_name, facts_list in facts_by_character_map.items():
                for fact_dict in facts_list:
                    fact_dict_with_owner = {**fact_dict, "character_name": char_name}
                    all_facts.append(IdentifiedFact(**fact_dict_with_owner))
            self.logger.info("âœ… æºæ•°æ®ä¸å¢å¼ºäº‹å®åŠ è½½å¹¶æ ¡éªŒæˆåŠŸã€‚")
            total_scenes = len(blueprint.scenes)

        except Exception as e:
            self.logger.error(f"âŒ ä¸¥é‡é”™è¯¯: åŠ è½½æˆ–è§£ææ–‡ä»¶æ—¶å¤±è´¥ã€‚\n   å…·ä½“é”™è¯¯: {e}", exc_info=True)
            raise

        self.logger.info("â–¶ï¸ æ­¥éª¤ 2/4: æ­£åœ¨å°†å¢å¼ºäº‹å®èåˆåˆ°å‰§æœ¬åœºæ™¯ä¸­...")
        facts_by_scene = defaultdict(list)
        for fact in all_facts:
            facts_by_scene[str(fact.scene_id)].append(fact)

        for scene_id, scene_obj in blueprint.scenes.items():
            if scene_id in facts_by_scene:
                scene_obj.enhanced_facts = facts_by_scene[scene_id]
        self.logger.info("âœ… æ•°æ®èåˆå®Œæˆã€‚")

        # ç¡®ä¿æœ¬åœ°æš‚å­˜ç›®å½•å­˜åœ¨ã€‚
        staging_dir.mkdir(parents=True, exist_ok=True)
        project_name = blueprint.project_metadata.project_name
        self.logger.info(f"â–¶ï¸ æ­¥éª¤ 3/4: æ­£åœ¨ä¸º '{project_name}' (Asset: {asset_id}) ç”Ÿæˆå¯Œæ–‡æœ¬æ–‡ä»¶...")

        # éå†æ¯ä¸ªåœºæ™¯ï¼Œè°ƒç”¨Pydanticæ¨¡å‹çš„æ–¹æ³•ç”ŸæˆRAGæ‰€éœ€çš„å¯Œæ–‡æœ¬å†…å®¹ã€‚
        for scene_id, scene_obj in blueprint.scenes.items():
            # [æ ¸å¿ƒä¿®æ”¹] ä¼ å…¥ asset_id (UUID) ä½œä¸º RAG æ–‡æ¡£çš„å…ƒæ•°æ®
            rich_text_content = scene_obj.to_rag_text(asset_id=asset_id, lang='zh')

            # [æ ¸å¿ƒä¿®æ”¹] æ–‡ä»¶åä½¿ç”¨ asset_id ç¡®ä¿å”¯ä¸€æ€§å’Œç¨³å®šæ€§
            # æ ¼å¼: {asset_id}_scene_{scene_id}_enhanced.txt
            scene_file_path = staging_dir / f"{asset_id}_scene_{scene_id}_enhanced.txt"
            scene_file_path.write_text(rich_text_content, encoding='utf-8')
        self.logger.info(f"âœ… å¯Œæ–‡æœ¬æ–‡æ¡£å·²åœ¨æœ¬åœ°æš‚å­˜ç›®å½• '{staging_dir}' ç”Ÿæˆã€‚")

        # æ„å»ºå¹¶è¿”å›GCSçš„ç›®æ ‡URIï¼Œç”¨äºåç»­çš„ä¸Šä¼ å’ŒRAGåŒæ­¥ã€‚
        gcs_uri = f"gs://{gcs_bucket_name}/rag-engine-source/{org_id}/{asset_id}"
        return gcs_uri, total_scenes

    def _upload_dir_to_gcs(self, local_dir: Path, gcs_uri: str):
        """å°†æœ¬åœ°ç›®å½•ä¸­çš„æ‰€æœ‰.txtæ–‡ä»¶ä¸Šä¼ åˆ°æŒ‡å®šçš„GCSè·¯å¾„ã€‚"""
        bucket_name = gcs_uri.split("/")[2]
        gcs_prefix = "/".join(gcs_uri.split("/")[3:])
        self.logger.info(f"â–¶ï¸ æ­¥éª¤ 4/4: æ­£åœ¨å°†æš‚å­˜ç›®å½•ä¸Šä¼ åˆ° GCS è·¯å¾„: '{gcs_uri}'...")
        try:
            storage_client = storage.Client(project=self.project_id)
            bucket = storage_client.bucket(bucket_name)

            # éå†æœ¬åœ°æš‚å­˜ç›®å½•ä¸­çš„æ‰€æœ‰txtæ–‡ä»¶å¹¶ä¸Šä¼ ã€‚
            for local_file in local_dir.glob("*.txt"):
                blob = bucket.blob(f"{gcs_prefix}/{local_file.name}")
                blob.upload_from_filename(str(local_file))
            self.logger.info(f"âœ… æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
        except Exception as e:
            self.logger.error(f"âŒ é”™è¯¯: ä¸Šä¼ åˆ°GCSå¤±è´¥: {e}", exc_info=True)
            raise

    def _deploy_to_rag_engine(self, corpus_display_name: str, gcs_uri: str):
        """åˆ›å»ºæˆ–æ›´æ–°RAGè¯­æ–™åº“ï¼Œå¹¶ä»GCSå¯¼å…¥æ–‡ä»¶ã€‚"""
        self.logger.info(f"â–¶ï¸ [æœ€ç»ˆæ­¥éª¤]: æ­£åœ¨å‘RAGè¯­æ–™åº“ '{corpus_display_name}' åŒæ­¥æ•°æ®...")
        try:
            # å¹‚ç­‰æ€§æ£€æŸ¥ï¼šé¦–å…ˆåˆ—å‡ºæ‰€æœ‰è¯­æ–™åº“ï¼ŒæŸ¥æ‰¾æ˜¯å¦å­˜åœ¨åŒåå®ä¾‹ã€‚
            corpora = rag.list_corpora()
            rag_corpus = next((c for c in corpora if c.display_name == corpus_display_name), None)

            # å¦‚æœè¯­æ–™åº“ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ã€‚
            if not rag_corpus:
                self.logger.info(f"   æœªæ‰¾åˆ°è¯­æ–™åº“ '{corpus_display_name}'ã€‚æ­£åœ¨åˆ›å»ºæ–°çš„è¯­æ–™åº“...")
                rag_corpus = rag.create_corpus(display_name=corpus_display_name)
                self.logger.info("âœ… æ–°è¯­æ–™åº“åˆ›å»ºæˆåŠŸã€‚")
            else:
                self.logger.info("âœ… RAGè¯­æ–™åº“å·²å­˜åœ¨ï¼Œå°†è¿›è¡Œæ–‡ä»¶åŒæ­¥/æ›´æ–°ã€‚")

            # å‘èµ·æ–‡ä»¶å¯¼å…¥è¯·æ±‚ã€‚è¿™æ˜¯ä¸€ä¸ªå¼‚æ­¥æ“ä½œï¼ŒAPIä¼šç«‹å³è¿”å›ã€‚
            # RAGå¼•æ“ä¼šåœ¨åå°ä»GCSæ‹‰å–ã€è§£æã€åˆ†å—å¹¶ç´¢å¼•æ–‡ä»¶ã€‚
            self.logger.info(f"   æ­£åœ¨ä» GCS URI: {gcs_uri} å‘èµ·æ–‡ä»¶å¯¼å…¥è¯·æ±‚...")
            rag.import_files(
                rag_corpus.name,
                [gcs_uri],
                transformation_config=rag.TransformationConfig(
                    chunking_config=rag.ChunkingConfig(chunk_size=512, chunk_overlap=50)
                )
            )
            self.logger.info("âœ… æ–‡ä»¶å¯¼å…¥è¯·æ±‚å·²æˆåŠŸå‘èµ·ã€‚")
        except Exception as e:
            self.logger.error(f"âŒ é”™è¯¯: å¤„ç†RAGè¯­æ–™åº“æ—¶å¤±è´¥: {e}", exc_info=True)
            raise