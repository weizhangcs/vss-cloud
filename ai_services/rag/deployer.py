# task_manager/rag_deployment/deployer.py

import json
from collections import defaultdict
from pathlib import Path
import logging # <-- ä½¿ç”¨æ ‡å‡†æ—¥å¿—åº“

# ä»æˆ‘ä»¬æ–°åˆ›å»ºçš„schemasæ¨¡å—å¯¼å…¥
from .schemas import NarrativeBlueprint, IdentifiedFact

import vertexai
from vertexai import rag
from google.cloud import storage
from django.conf import settings # <-- å¯¼å…¥Django settings

# è·å–ä¸€ä¸ªæ—¥å¿—è®°å½•å™¨å®ä¾‹
logger = logging.getLogger(__name__)

class RagDeployer:
    """
    ä¸€ä¸ªè¢«æ”¹é€ åã€é€‚é…Django Celeryç¯å¢ƒçš„RAGéƒ¨ç½²å™¨ã€‚
    """
    def __init__(self):
        """
        åˆå§‹åŒ–æ—¶ï¼Œç›´æ¥ä»Django settingsåŠ è½½é…ç½®ã€‚
        """
        self.project_id = settings.GOOGLE_CLOUD_PROJECT
        self.location = settings.GOOGLE_CLOUD_LOCATION

        # åˆå§‹åŒ–Vertex AI
        vertexai.init(project=self.project_id, location=self.location)
        logger.info(f"RagDeployer initialized for project '{self.project_id}' in '{self.location}'")

    def execute(self, instance_id: str, blueprint_path: Path, facts_path: Path, gcs_bucket: str, corpus_basename: str,
                staging_dir: Path):
        """æ‰§è¡Œå®Œæ•´çš„éƒ¨ç½²æµç¨‹ã€‚"""
        corpus_full_name = f"{corpus_basename}-{instance_id}"
        logger.info("=" * 20 + f" ğŸš€ RAG éƒ¨ç½²ä»»åŠ¡å¯åŠ¨ (å®ä¾‹: {instance_id}) ğŸš€ " + "=" * 20)

        try:
            gcs_uri = self._fuse_and_prepare_files(
                source_blueprint_path=blueprint_path,
                enhanced_facts_path=facts_path,
                staging_dir=staging_dir,
                gcs_bucket_name=gcs_bucket,
                instance_id=instance_id
            )

            self._upload_dir_to_gcs(
                local_dir=staging_dir,
                gcs_uri=gcs_uri,
            )

            self._deploy_to_rag_engine(
                corpus_full_name=corpus_full_name,
                gcs_uri=gcs_uri
            )

            logger.info("=" * 70)
            logger.info(f"âœ… å®ä¾‹ '{instance_id}' çš„RAGéƒ¨ç½²ä»»åŠ¡å·²æˆåŠŸå¯åŠ¨ï¼")
            logger.info(f"   ç›®æ ‡è¯­æ–™åº“: {corpus_full_name}")
            logger.info("   è¯·å‰å¾€Google Cloudæ§åˆ¶å°æŸ¥çœ‹æ–‡ä»¶å¯¼å…¥è¿›åº¦ã€‚")
            logger.info("=" * 70 + "\n")

        except Exception as e:
            self.logger.critical(f"éƒ¨ç½²æµç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            # å¯ä»¥åœ¨è¿™é‡Œå†³å®šæ˜¯å¦è¦é‡æ–°æŠ›å‡ºå¼‚å¸¸
            # raise

    def _fuse_and_prepare_files(self, source_blueprint_path: Path, enhanced_facts_path: Path, staging_dir: Path,
                                gcs_bucket_name: str, instance_id: str) -> str:
        logger.info(f"â–¶ï¸ æ­¥éª¤ 1/4: æ­£åœ¨åŠ è½½å®ä¾‹ '{instance_id}' çš„æºæ•°æ®...")
        try:
            blueprint = NarrativeBlueprint.parse_file(source_blueprint_path)
            with enhanced_facts_path.open('r', encoding='utf-8') as f:
                facts_data = json.load(f)
            all_facts = []
            facts_by_character_map = facts_data.get("identified_facts_by_character", {})
            for char_name, facts_list in facts_by_character_map.items():
                for fact_dict in facts_list:
                    fact_dict_with_owner = {**fact_dict, "character_name": char_name}
                    all_facts.append(IdentifiedFact(**fact_dict_with_owner))
            logger.info("âœ… æºæ•°æ®ä¸å¢å¼ºäº‹å®åŠ è½½å¹¶æ ¡éªŒæˆåŠŸã€‚")
        except Exception as e:
            self.logger.error(f"âŒ ä¸¥é‡é”™è¯¯: åŠ è½½æ–‡ä»¶æ—¶å¤±è´¥ã€‚\n   å…·ä½“é”™è¯¯: {e}")
            raise e

        logger.info("â–¶ï¸ æ­¥éª¤ 2/4: æ­£åœ¨èåˆå¢å¼ºäº‹å®...")
        facts_by_scene = defaultdict(list)
        for fact in all_facts:
            facts_by_scene[str(fact.scene_id)].append(fact)
        for scene_id, scene_obj in blueprint.scenes.items():
            if scene_id in facts_by_scene:
                scene_obj.enhanced_facts = facts_by_scene[scene_id]
        logger.info("âœ… æ•°æ®èåˆå®Œæˆã€‚")

        staging_dir.mkdir(parents=True, exist_ok=True)
        series_id = blueprint.project_metadata.project_name
        logger.info(f"â–¶ï¸ æ­¥éª¤ 3/4: æ­£åœ¨ä¸º '{series_id}' ç”Ÿæˆå¯Œæ–‡æœ¬æ–‡ä»¶...")
        for scene_id, scene_obj in blueprint.scenes.items():
            rich_text_content = scene_obj.to_rag_b_text(series_id=series_id, lang='zh')
            scene_file_path = staging_dir / f"{series_id}_scene_{scene_id}_enhanced.txt"
            scene_file_path.write_text(rich_text_content, encoding='utf-8')
        logger.info(f"âœ… å¯Œæ–‡æœ¬æ–‡æ¡£å·²åœ¨æœ¬åœ°æš‚å­˜ç›®å½• '{staging_dir}' ç”Ÿæˆã€‚")

        gcs_uri = f"gs://{gcs_bucket_name}/rag-engine-source/{instance_id}/{series_id}"
        return gcs_uri

    def _upload_dir_to_gcs(self, local_dir: Path, gcs_uri: str):
        bucket_name = gcs_uri.split("/")[2]
        gcs_prefix = "/".join(gcs_uri.split("/")[3:])
        logger.info(f"â–¶ï¸ æ­¥éª¤ 4/4: æ­£åœ¨å°†æš‚å­˜ç›®å½•ä¸Šä¼ åˆ° GCS è·¯å¾„: '{gcs_uri}'...")
        try:
            storage_client = storage.Client(project=self.project_id)
            bucket = storage_client.bucket(bucket_name)
            for local_file in local_dir.glob("*.txt"):
                blob = bucket.blob(f"{gcs_prefix}/{local_file.name}")
                blob.upload_from_filename(str(local_file))
            logger.info(f"âœ… æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
        except Exception as e:
            self.logger.error(f"âŒ é”™è¯¯: ä¸Šä¼ åˆ°GCSå¤±è´¥: {e}")
            raise e

    def _deploy_to_rag_engine(self, corpus_full_name: str, gcs_uri: str):
        logger.info(f"â–¶ï¸ [æœ€ç»ˆæ­¥éª¤]: æ­£åœ¨å‘RAGè¯­æ–™åº“ '{corpus_full_name}' åŒæ­¥æ•°æ®...")
        try:
            corpora = rag.list_corpora()
            rag_corpus = next((c for c in corpora if c.display_name == corpus_full_name), None)
            if not rag_corpus:
                logger.info(f"   æœªæ‰¾åˆ°è¯­æ–™åº“ '{corpus_full_name}'ã€‚æ­£åœ¨åˆ›å»ºæ–°çš„è¯­æ–™åº“...")
                rag_corpus = rag.create_corpus(display_name=corpus_full_name)
                logger.info("âœ… æ–°è¯­æ–™åº“åˆ›å»ºæˆåŠŸã€‚")
            else:
                logger.info("âœ… RAGè¯­æ–™åº“å·²å­˜åœ¨ã€‚")
            logger.info(f"   æ­£åœ¨ä» GCS URI: {gcs_uri} å‘èµ·æ–‡ä»¶å¯¼å…¥è¯·æ±‚...")
            rag.import_files(
                rag_corpus.name,
                [gcs_uri],
                transformation_config=rag.TransformationConfig(
                    chunking_config=rag.ChunkingConfig(chunk_size=512, chunk_overlap=50)
                )
            )
            logger.info("âœ… æ–‡ä»¶å¯¼å…¥è¯·æ±‚å·²æˆåŠŸå‘èµ·ã€‚")
        except Exception as e:
            self.logger.error(f"âŒ é”™è¯¯: å¤„ç†RAGè¯­æ–™åº“æ—¶å¤±è´¥: {e}")
            raise e