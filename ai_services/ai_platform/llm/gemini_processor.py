# ai_services/common/gemini/gemini_processor.py
# æè¿°: å¢å¼ºç‰ˆçš„ Gemini API å¤„ç†å™¨ï¼Œç”¨äºå¤„ç†åŒæ­¥/å¼‚æ­¥è¯·æ±‚ã€é‡è¯•å’Œ JSON æå–ã€‚
# ç‰ˆæœ¬: 1.0 (é‡æ„ç‰ˆ - åŸºäº genai.Client)

import json
import re
import time
import inspect
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, Union, List, Callable, Awaitable, Tuple
import logging

from google import genai
from google.api_core import exceptions
from google.genai.errors import ServerError
from google.genai import types

from core.exceptions import RateLimitException

class GeminiProcessor:
    """
    [å·²é‡æ„] å¢å¼ºç‰ˆçš„Gemini APIå¤„ç†å™¨ã€‚
    è´Ÿè´£å°è£…ä¸ Google Gemini API çš„æ‰€æœ‰äº¤äº’é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
    - åˆå§‹åŒ– genai.Client å®ä¾‹ã€‚
    - ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨é‡è¯•æœºåˆ¶ã€‚
    - ä» Markdown å›´æ ä¸­å®‰å…¨æå– JSON å“åº”ã€‚
    - è®°å½•è¯·æ±‚å’Œå“åº”æ—¥å¿— (è°ƒè¯•æ¨¡å¼)ã€‚
    """

    # --- é™æ€é…ç½®å¸¸é‡ ---
    _MAX_RETRIES = 3
    _INITIAL_RETRY_DELAY = 1
    _MAX_RETRY_DELAY = 10
    # å®šä¹‰å¯é‡è¯•çš„ API é”™è¯¯ç±»å‹ (ä¾‹å¦‚ï¼šæœåŠ¡å™¨é”™è¯¯ã€æœåŠ¡ä¸å¯ç”¨ã€é€Ÿç‡é™åˆ¶)
    _RETRYABLE_ERRORS = (
        exceptions.ServiceUnavailable, ServerError, exceptions.TooManyRequests,
        exceptions.InternalServerError, exceptions.GatewayTimeout,
    )

    def __init__(self, api_key: str, logger: logging.Logger, debug_mode: bool = False, debug_dir: Union[str, Path] = "gemini_debug", caller_class: Optional[str] = None):
        """
        åˆå§‹åŒ–æ—¶ï¼Œæ¥æ”¶æ‰€æœ‰å¿…è¦çš„é…ç½®ä½œä¸ºå‚æ•°ï¼Œå¹¶åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹ã€‚

        Args:
            api_key (str): Google Gemini API Keyã€‚
            logger (logging.Logger): æ—¥å¿—è®°å½•å™¨å®ä¾‹ (é€šè¿‡ä¾èµ–æ³¨å…¥ä¼ å…¥)ã€‚
            debug_mode (bool): æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜è¯·æ±‚/å“åº”æ—¥å¿—ã€‚
            debug_dir (Union[str, Path]): è°ƒè¯•æ—¥å¿—çš„ä¿å­˜ç›®å½•ã€‚
            caller_class (Optional[str]): è°ƒç”¨è¯¥å¤„ç†å™¨çš„æœåŠ¡ç±»åã€‚
        """
        if not api_key:
            raise ValueError("API Key ä¸èƒ½ä¸ºç©ºã€‚")

        self.api_key = api_key
        self.logger = logger
        self.debug_mode = debug_mode
        self.debug_dir = Path(debug_dir)

        # [ä¿®æ”¹] 2. å¢åŠ è·¯å¾„è‡ªåŠ¨æ”¶æ•›é€»è¾‘
        if self.debug_mode:
            if debug_dir:
                self.debug_dir = Path(debug_dir)
            else:
                # å¦‚æœå¼€å¯è°ƒè¯•ä½†æœªæŒ‡å®šè·¯å¾„ï¼Œå¼ºåˆ¶æ”¶æ•›åˆ° shared_media/logs/gemini_debug
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾è¿è¡Œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼Œæˆ–è€…é€šè¿‡ç›¸å¯¹è·¯å¾„è®¿é—®
                self.debug_dir = Path("shared_media/logs/gemini_debug")
                # å¯é€‰ï¼šæ‰“å°ä¸€æ¡è­¦å‘Šæ—¥å¿—ï¼Œæç¤ºä½¿ç”¨äº†é»˜è®¤è·¯å¾„
                # self.logger.warning(f"Debug mode on but no path provided. Using default: {self.debug_dir}")

            self.log_dir = self.debug_dir
        else:
            self.debug_dir = None
            self.log_dir = None

        self.caller_class = caller_class or self._get_caller_class_name()
        # åˆ›å»ºä¸€ä¸ªåŸºäºè°ƒç”¨è€…å’Œæ—¶é—´çš„ä¼šè¯IDï¼Œç”¨äºæ—¥å¿—æ–‡ä»¶å
        self.session_id = f"{self.caller_class}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # æ ¸å¿ƒï¼šä½¿ç”¨ genai.Client å®ä¾‹åŒ–ï¼Œè´Ÿè´£ç®¡ç†è¿æ¥
        try:
            self._client = genai.Client(api_key=self.api_key)
            self.logger.info("GeminiProcessor initialized and genai.Client created successfully.")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ– genai.Client æ—¶å¤±è´¥: {e}", exc_info=True)
            raise

    def _build_generation_config(self, model_name: str, temperature: Optional[float] = None,
                                 tools: Optional[List] = None) -> Optional[types.GenerateContentConfig]:
        """
        [Final Strategic Fix] ç§»é™¤ JSON Mode ç¡¬çº¦æŸï¼Œè§£é™¤ä¸ AFC çš„æ­»é”
        """

        # 1. åŸºç¡€é…ç½®ï¼šä¸å†å¼ºåˆ¶ response_mime_type="application/json"
        # æˆ‘ä»¬ä¾é  Prompt å’Œ Regex Parser æ¥ä¿è¯ JSON æ ¼å¼
        config_params = {}

        # 2. å®‰å…¨è®¾ç½® (ä¿ç•™ BLOCK_NONEï¼Œè¿™å¯¹ Batch 5 å¾ˆé‡è¦)
        # ä½¿ç”¨åŸç”Ÿå­—å…¸åˆ—è¡¨
        safety_settings = [
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        ]
        config_params["safety_settings"] = safety_settings

        # 3. æ¸©åº¦æ§åˆ¶
        if temperature is not None:
            config_params['temperature'] = temperature

        # 4. é’ˆå¯¹ Gemini 3.0 çš„å¤„ç† (3.0 ä¾ç„¶å¯ä»¥å°è¯• JSON Modeï¼Œæˆ–è€…ä¹Ÿé™çº§)
        # ä¸ºäº†ç¨³å¦¥ï¼Œå»ºè®®å¯¹ 2.5 å’Œ 3.0 éƒ½ç»Ÿä¸€ç­–ç•¥ï¼šä¸å¼ºåˆ¶ JSON Mode
        if "gemini-3" in model_name:
            # å³ä½¿æ˜¯ 3.0ï¼Œå¦‚æœç¯å¢ƒé‡Œæœ‰ AFC å¹²æ‰°ï¼ŒJSON Mode ä¹Ÿå¯èƒ½å¯¼è‡´ä¸ç¨³å®š
            # æ‰€ä»¥è¿™é‡Œä¹Ÿå»æ‰ response_mime_type

            target_level = "high"
            if temperature is not None and temperature < 0.3:
                target_level = "low"

            return types.GenerateContentConfig(
                # response_mime_type="application/json", <--- åˆ é™¤è¿™è¡Œ
                safety_settings=safety_settings,
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False,
                    thinking_level=target_level
                )
            )

        # 5. Legacy æ¨¡å‹ (Gemini 2.5)
        else:
            # åªè¦ config é‡Œæ²¡æœ‰ response_mime_typeï¼Œä¹Ÿæ²¡æœ‰ tools (æˆ– tools=None)
            # å°±ç®— SDK é»˜è®¤å¸¦äº† AFCï¼Œæ™®é€š Text Mode ä¹Ÿä¸ä¼šå´©æºƒ
            return types.GenerateContentConfig(**config_params)

    def _extract_clean_text(self, response) -> str:
        """
        [è¯Šæ–­æ¨¡å¼] æ·±åº¦æ‰“å° API å“åº”çš„å†…éƒ¨ç»“æ„
        """
        text_parts = []
        try:
            # 1. æ£€æŸ¥ Candidates æ˜¯å¦å­˜åœ¨
            if not response.candidates:
                self.logger.error("âŒ DIAGNOSTIC: No candidates returned! (Empty Response)")
                return ""

            candidate = response.candidates[0]

            # 2. æ‰“å°å…³é”®çš„ Finish Reason (è¿™æ˜¯ç ´æ¡ˆçš„å…³é”®)
            # æ­£å¸¸åº”è¯¥æ˜¯ STOPã€‚å¦‚æœæ˜¯ SAFETY, RECITATION, æˆ– OTHERï¼Œé‚£å°±æ˜¯è¢«æ‹¦æˆªäº†ã€‚
            finish_reason = getattr(candidate, 'finish_reason', 'UNKNOWN')
            self.logger.info(f"ğŸ” DIAGNOSTIC: Finish Reason = {finish_reason}")

            # 3. æ£€æŸ¥æ˜¯å¦è§¦å‘äº† Function Call (AFC å¹½çµ)
            for part in candidate.content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    self.logger.error(f"âŒ DIAGNOSTIC: Model tried to call a function! Name: {part.function_call.name}")
                    # å¦‚æœå®ƒè¯•å›¾è°ƒç”¨å‡½æ•°ï¼Œè¯´æ˜ Prompt æˆ– Tools é…ç½®æœ‰é—®é¢˜
                    return ""

                if hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)

            # 4. å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œæ‰“å°æ•´ä¸ª Candidate ç»“æ„
            if not text_parts:
                self.logger.error(f"âŒ DIAGNOSTIC: No text parts found. Full Candidate dump: {candidate}")

        except Exception as e:
            self.logger.error(f"Diagnostic extraction failed: {e}")
            return ""

        return "".join(text_parts)

    def generate_content(
            self,
            model_name: str,
            prompt: Union[str, List],
            stream: bool = False,
            temperature: Optional[float] = None,
            tools: Optional[List] = None,
            tool_config: Optional[Any] = None,
            **generation_kwargs
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        æ‰§è¡ŒåŒæ­¥çš„ AI å†…å®¹ç”Ÿæˆè¯·æ±‚ (å·²å‡çº§é€‚é… Gemini 3)ã€‚
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')

        # [ä¿®æ”¹ 1] è°ƒç”¨æ™ºèƒ½é…ç½®æ„å»ºå™¨
        config = self._build_generation_config(model_name, temperature, tools=tools)

        # åˆå¹¶ kwargs ä¸­çš„é¢å¤–å‚æ•° (å¦‚æœæœ‰)
        # æ³¨æ„ï¼šgeneration_kwargs ä¸­çš„å†²çªå‚æ•°å¯èƒ½éœ€è¦æ¸…ç†ï¼Œè¿™é‡Œæš‚æ—¶ç•¥è¿‡

        request_log = {
            "model": model_name,
            "prompt": prompt, # ç”Ÿäº§ç¯å¢ƒå»ºè®®æˆªæ–­ prompt æ—¥å¿—
            "config_dump": str(config) if config else "None",  # è®°å½•ä¸€ä¸‹ Config æ–¹ä¾¿è°ƒè¯•
            "timestamp": timestamp,
            "caller": self.caller_class
        }
        self._log_to_file("requests", "request_", request_log)

        start_time = datetime.now()
        if tools is None:
            generation_kwargs.pop('tools', None)
            generation_kwargs.pop('tool_config', None)

        try:
            # å®šä¹‰ API è°ƒç”¨çš„å‡½æ•°å¥æŸ„
            api_call = lambda: self._client.models.generate_content(
                model=model_name, contents=prompt,config=config
            )
            response = self._retry_api_call(api_call, "åŒæ­¥ç”Ÿæˆ")

            # [ä¿®æ”¹ 2] ä½¿ç”¨å®‰å…¨æå–æ–¹æ³•ï¼Œæ›¿ä»£ response.text
            full_response_text = self._extract_clean_text(response)

            # æå– Tokens ç”¨é‡ (Gemini 3 çš„ç»“æ„å¯èƒ½ç•¥æœ‰ä¸åŒï¼Œå»ºè®®åŠ  getattr é˜²å¾¡)
            usage_meta = getattr(response, 'usage_metadata', None)
            usage = {
                "model_used": model_name,
                "prompt_tokens": getattr(usage_meta, 'prompt_token_count', 0),
                "completion_tokens": getattr(usage_meta, 'candidates_token_count', 0),
                "total_tokens": getattr(usage_meta, 'total_token_count', 0)
            }

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            usage.update({
                "start_time_utc": start_time.isoformat(),
                "end_time_utc": end_time.isoformat(),
                "duration_seconds": round(duration, 4),
                "request_count": 1
            })

            self._log_to_file("raw_responses", "raw_", full_response_text)
            parsed_data = self._parse_json_response(full_response_text)
            self._log_to_file("parsed_responses", "parsed_", {
                "data": parsed_data, "usage": usage, "timestamp": timestamp
            })

            return parsed_data, usage

        except Exception as e:
            self._log_and_raise(e, "ç”Ÿæˆå†…å®¹")

    def count_tokens(self, contents: Union[str, List], model_name: str) -> int:
        """è®¡ç®—ç»™å®šå†…å®¹åœ¨ç‰¹å®šæ¨¡å‹ä¸‹çš„ token æ•°é‡ã€‚"""
        try:
            response = self._client.models.count_tokens(model=model_name, contents=contents)
            return response.total_tokens
        except Exception as e:
            self._log_to_file("errors", "token_count_error_", {
                "error": str(e),
                "contents": contents[:200] if isinstance(contents, str) else contents,
                "model": model_name
            })
            raise RuntimeError(f"Tokenè®¡æ•°å¤±è´¥: {str(e)}") from e

    def _retry_api_call(self, api_func: Callable, context: str) -> Any:
        """
        åŒæ­¥ API è°ƒç”¨çš„é‡è¯•åŒ…è£…å™¨ï¼Œå®ç°æŒ‡æ•°é€€é¿å’Œé”™è¯¯æ•è·ã€‚
        """
        last_exception = None
        for attempt in range(self._MAX_RETRIES + 1):
            try:
                result = api_func()
                # é‡è¯•æˆåŠŸåæ‰“å°æç¤ºä¿¡æ¯
                if attempt > 0:
                    print(f"âœ… APIè°ƒç”¨é‡è¯•æˆåŠŸ (åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•)ã€‚ç»§ç»­æ‰§è¡Œ...")
                return result
            except self._RETRYABLE_ERRORS as e:
                last_exception = e
                if attempt < self._MAX_RETRIES:
                    delay = min(self._INITIAL_RETRY_DELAY * (2 ** attempt), self._MAX_RETRY_DELAY)
                    print(
                        f"APIè°ƒç”¨å¤±è´¥ ({type(e).__name__})ï¼Œå°†åœ¨ {delay} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{self._MAX_RETRIES})")
                    time.sleep(delay)
                    continue
                else:
                    print(f"APIè°ƒç”¨åœ¨ {self._MAX_RETRIES} æ¬¡é‡è¯•åå½»åº•å¤±è´¥ã€‚")
                    self._log_and_raise(e, f"{context} (é‡è¯• {self._MAX_RETRIES} æ¬¡å)")
        raise last_exception

    async def _retry_api_call_async(self, api_func_awaitable: Callable[[], Awaitable], context: str) -> Any:
        """
        å¼‚æ­¥ API è°ƒç”¨çš„é‡è¯•åŒ…è£…å™¨ (ä¸åŒæ­¥ç‰ˆæœ¬é€»è¾‘ç±»ä¼¼)ã€‚
        """
        last_exception = None
        for attempt in range(self._MAX_RETRIES + 1):
            try:
                result = await api_func_awaitable()
                # å¼‚æ­¥æ–¹æ³•åŒæ ·å¢åŠ æˆåŠŸæç¤º
                if attempt > 0:
                    print(f"âœ… å¼‚æ­¥APIè°ƒç”¨é‡è¯•æˆåŠŸ (åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•)ã€‚ç»§ç»­æ‰§è¡Œ...")
                return result
            except self._RETRYABLE_ERRORS as e:
                last_exception = e
                if attempt < self._MAX_RETRIES:
                    delay = min(self._INITIAL_RETRY_DELAY * (2 ** attempt), self._MAX_RETRY_DELAY)
                    print(
                        f"å¼‚æ­¥APIè°ƒç”¨å¤±è´¥ ({type(e).__name__})ï¼Œå°†åœ¨ {delay} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{self._MAX_RETRIES})")
                    await asyncio.sleep(delay)
                    continue
                else:
                    print(f"å¼‚æ­¥APIè°ƒç”¨åœ¨ {self._MAX_RETRIES} æ¬¡é‡è¯•åå½»åº•å¤±è´¥ã€‚")
                    self._log_and_raise(e, f"{context} (é‡è¯• {self._MAX_RETRIES} æ¬¡å)")
        raise last_exception

    async def generate_content_async(
            self,
            model_name: str,
            prompt: Union[str, List],
            **generation_kwargs
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        æ‰§è¡Œå¼‚æ­¥çš„ AI å†…å®¹ç”Ÿæˆè¯·æ±‚ã€‚

        ã€é¢„ç•™ç›®çš„è¯´æ˜ã€‘:
        æ­¤æ–¹æ³•ç”¨äº Django ASGI æˆ– asyncio å¹¶å‘åœºæ™¯ã€‚
        åœ¨ Gemini Developer API çš„é€Ÿç‡é™åˆ¶çº¦æŸä¸‹ï¼ŒåŒæ­¥æ–¹æ³• (`generate_content`) é…åˆå†…ç½®é‡è¯•å·²è¶³å¤Ÿé«˜æ•ˆã€‚
        æœ¬æ–¹æ³•é¢„ç•™ç»™æœªæ¥éœ€è¦**å¹¶è¡Œæ‰¹å¤„ç†**æˆ–**é«˜å¹¶å‘ API è§†å›¾**æ—¶ä½¿ç”¨ã€‚
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')

        # å‡†å¤‡æ—¥å¿—è®°å½•çš„å‚æ•° (å¤„ç† config object ç­‰)
        log_kwargs = {k: dict(v) if isinstance(v, types.GenerateContentConfig) else v for k, v in
                      generation_kwargs.items()}

        request_log = {
            "model": model_name, "prompt": prompt, "kwargs": log_kwargs,
            "timestamp": timestamp, "caller": self.caller_class
        }
        self._log_to_file("requests_async", "request_", request_log)

        try:
            # å®šä¹‰å¼‚æ­¥ API è°ƒç”¨çš„å‡½æ•°å¥æŸ„
            api_call = lambda: self._client.aio.models.generate_content(
                model=model_name, contents=prompt, **generation_kwargs
            )
            response = await self._retry_api_call_async(api_call, "å¼‚æ­¥ç”Ÿæˆ")

            full_response_text = response.text
            self._log_to_file("raw_responses_async", "raw_", full_response_text)

            # æå– Tokens ç”¨é‡
            usage = {
                "model_used": model_name,
                "prompt_tokens": response.usage_metadata.prompt_token_count,
                "completion_tokens": response.usage_metadata.candidates_token_count,
                "total_tokens": response.usage_metadata.total_token_count,
            }

            parsed_data = self._parse_json_response(full_response_text)
            self._log_to_file("parsed_responses_async", "parsed_", {
                "data": parsed_data, "usage": usage, "timestamp": timestamp
            })

            return parsed_data, usage

        except Exception as e:
            self._log_and_raise(e, "å¼‚æ­¥ç”Ÿæˆ")

    def _get_caller_class_name(self) -> str:
        """é€šè¿‡æ£€æŸ¥è°ƒç”¨æ ˆï¼Œè‡ªåŠ¨æ£€æµ‹è°ƒç”¨è¯¥å¤„ç†å™¨çš„ä¸Šå±‚ç±»åï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚"""
        frame = inspect.currentframe()
        try:
            # éå†è°ƒç”¨æ ˆï¼Œç›´åˆ°æ‰¾åˆ°åŒ…å« 'self' å®ä¾‹çš„å¸§
            while frame:
                frame = frame.f_back
                if not frame:
                    break
                if 'self' in frame.f_locals:
                    instance = frame.f_locals['self']
                    if hasattr(instance, '__class__'):
                        return instance.__class__.__name__
            return self.__class__.__name__  # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›å½“å‰ç±»å
        finally:
            del frame

    def _log_and_raise(self, e: Exception, context: str) -> None:
        """
        è¾…åŠ©å‡½æ•°ï¼šè®°å½•é”™è¯¯æ—¥å¿—ï¼Œå¹¶é‡æ–°æŠ›å‡ºå¼‚å¸¸ã€‚
        [ä¿®æ”¹] å¢å¼ºï¼šè¯†åˆ«é™æµé”™è¯¯å¹¶æŠ›å‡ºç‰¹å®šå¼‚å¸¸ï¼Œä¾›ä¸Šå±‚ Task æ•è·ã€‚
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
        error_msg = str(e)

        error_info = {
            "error_type": type(e).__name__,
            "error_message": error_msg,
            "timestamp": timestamp,
            "context": context,
            "stack_trace": self._get_clean_stacktrace()
        }
        log_subdir = "errors_async" if "async" in context else "errors"
        self._log_to_file(log_subdir, "error_", error_info)

        # --- æŠ›å‡ºå¼ºç±»å‹å¼‚å¸¸ ---
        # è¯†åˆ« Google/Aliyun å¸¸è§çš„é™æµå…³é”®å­—
        if any(k in error_msg for k in ["429", "ResourceExhausted", "Too Many Requests", "Throttling"]):
            print(f"âš ï¸ Detected Rate Limit in {context}. Raising RateLimitException.")
            raise RateLimitException(msg=error_msg, provider="Gemini/External") from e

        # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œä¿æŒæŠ›å‡º RuntimeError
        raise RuntimeError(f"{context}å¤±è´¥: {error_msg}") from e

    def _log_to_file(self, subdir: str, prefix: str, content: Any) -> Optional[Path]:
        """å°†è¯·æ±‚/å“åº”æ•°æ®æˆ–é”™è¯¯ä¿¡æ¯å†™å…¥è°ƒè¯•æ–‡ä»¶ (å¦‚æœ debug_mode å¼€å¯)ã€‚"""
        if not self.debug_mode or not self.log_dir:
            return None

        log_dir = self.log_dir / subdir
        log_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
        filename = f"{prefix}{timestamp}.json"
        filepath = log_dir / filename

        try:
            sanitized = self._sanitize_content(content)  # ç§»é™¤æ•æ„Ÿä¿¡æ¯
            with open(filepath, "w", encoding="utf-8") as f:
                if isinstance(sanitized, (dict, list)):
                    json.dump(sanitized, f, indent=2, ensure_ascii=False)
                else:
                    f.write(str(sanitized))
            return filepath
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—è®°å½•å¤±è´¥({filepath}): {str(e)}")
            return None

    def _sanitize_content(self, content: Any) -> Any:
        """
        ä»å†…å®¹ä¸­ç§»é™¤ API key æˆ– secret ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œ
        [æ–°å¢] å¹¶å°†æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¦‚å›¾ç‰‡ï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²å ä½ç¬¦ã€‚
        """
        if isinstance(content, dict):
            content = content.copy()
            for key in list(content.keys()):
                val = content[key]
                # 1. æ•æ„Ÿä¿¡æ¯è„±æ•
                if "key" in key.lower() or "secret" in key.lower():
                    content[key] = "***REDACTED***"
                # 2. [æ–°å¢] é€’å½’å¤„ç†åµŒå¥—å­—å…¸
                else:
                    content[key] = self._sanitize_content(val)

        elif isinstance(content, list):
            # [æ–°å¢] å¤„ç†åˆ—è¡¨ä¸­çš„å›¾ç‰‡å¯¹è±¡
            return [self._sanitize_content(item) for item in content]

        # [æ–°å¢] æ£€æŸ¥æ˜¯å¦æ˜¯ PIL Image å¯¹è±¡ (é€šè¿‡ç±»ååˆ¤æ–­ï¼Œé¿å…å¼•å…¥ PIL ä¾èµ–)
        elif hasattr(content, '__class__') and 'Image' in content.__class__.__name__:
             return f"<Image Object: {content.__class__.__name__}>"

        return content

    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """
        ä» LLM è¿”å›çš„æ–‡æœ¬ä¸­å®‰å…¨åœ°æå–å’Œè§£æ JSON å¯¹è±¡ã€‚

        å®ƒé¦–å…ˆå°è¯•åŒ¹é… Markdown JSON å›´æ ä¸­çš„å†…å®¹ï¼Œå¹¶å°è¯•ä¿®å¤å¸¸è§çš„å°¾éšé€—å·é”™è¯¯ã€‚
        """
        # 1. å°è¯•åŒ¹é… Markdown JSON å›´æ  (ä¾‹å¦‚: ```json{...}```)
        match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", text, re.DOTALL)
        json_str = match.group(1) if match else text

        # 2. å°è¯•ä¿®å¤å°¾éšé€—å· (ä¾‹å¦‚: {"a": 1,})
        json_str_fixed = re.sub(r',\s*([}\]])', r'\1', json_str)

        try:
            # 3. å°è¯•è§£æä¿®å¤åçš„å­—ç¬¦ä¸²
            return json.loads(json_str_fixed)
        except json.JSONDecodeError:
            try:
                # 4. å¦‚æœå¤±è´¥ï¼Œå°è¯•è§£æåŸå§‹æå–çš„å­—ç¬¦ä¸² (å¯èƒ½ä¿®å¤ä¸éœ€è¦)
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                # 5. æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•å¹¶æŠ›å‡ºå¼‚å¸¸
                self._log_to_file("errors", "parsing_error_", {
                    "error": "Final JSON parsing failed after all fix attempts.",
                    "original_error": str(e),
                    "original_snippet": text[:500],
                    "processed_snippet": json_str_fixed[:500]
                })
                raise ValueError(f"JSONè§£æå¤±è´¥: {e}\nç‰‡æ®µ: {json_str[:200]}...")

    def _get_clean_stacktrace(self) -> List[str]:
        """è·å–å¹¶æ¸…ç†è°ƒç”¨æ ˆä¿¡æ¯ï¼Œæ’é™¤å¤„ç†å™¨æœ¬èº«çš„å†…éƒ¨å¸§ï¼Œä»¥æä¾›æ›´æ¸…æ™°çš„é”™è¯¯æº¯æºã€‚"""
        stack = []
        for frame_info in inspect.stack():
            # æ’é™¤ä¸å½“å‰æ–‡ä»¶ç›¸å…³çš„å†…éƒ¨è°ƒç”¨
            if "gemini_processor" in frame_info.filename.lower():
                continue
            stack.append(f"{frame_info.filename}:{frame_info.lineno} ({frame_info.function})")
        return stack