# ai_services/ai_platform/rag/deployer.py

import json
import logging
from collections import defaultdict
from pathlib import Path
from typing import List

import vertexai
from google.api_core import exceptions as google_exceptions
from core.exceptions import RateLimitException

# [Import ä¿®æ­£]
from ai_services.biz_services.narrative_dataset import NarrativeDataset
from .corpus_manager import CorpusManager
from .data_manager import DataManager
from .schemas import IdentifiedFact, RagContentFormatter, CharacterFactsFile

from file_service.infrastructure.gcs_storage import upload_directory_to_gcs


class RagDeployer:
    """
    RAG éƒ¨ç½²å™¨æœåŠ¡ (V6 Dataset Adapter ç‰ˆ)
    """

    def __init__(self, project_id: str, location: str, logger: logging.Logger):
        self.project_id = project_id
        self.location = location
        self.logger = logger

        try:
            vertexai.init(project=self.project_id, location=self.location)
            self.corpus_manager = CorpusManager()
            self.data_manager = DataManager()
            self.logger.info(f"RagDeployer initialized (Project: {project_id})")
        except Exception as e:
            self.logger.error(f"Vertex AI initialization failed: {e}", exc_info=True)
            raise

    def execute(self,
                corpus_display_name: str,
                dataset_obj: NarrativeDataset,
                facts_path: Path,
                gcs_bucket_name: str,
                staging_dir: Path,
                org_id: str,
                asset_id: str,
                i18n_schema_path: Path,  # [æ–°å¢] æ¥æ”¶ Schema è·¯å¾„
                lang: str = 'zh'
                ):

        self.logger.info("=" * 20 + f" ğŸš€ RAG éƒ¨ç½²ä»»åŠ¡å¯åŠ¨ (V6) " + "=" * 20)

        # [Step 0: åŠ è½½ i18n é…ç½®]
        # è¿™é‡Œç”± Deployer è´Ÿè´£åŠ è½½èµ„æºï¼ŒèŒè´£å½’å±æ›´æ¸…æ™°
        i18n_labels = {}
        try:
            with i18n_schema_path.open('r', encoding='utf-8') as f:
                full_i18n = json.load(f)
                i18n_labels = full_i18n.get(lang, full_i18n.get('en', {}))
        except Exception as e:
            self.logger.warning(f"Failed to load i18n labels from {i18n_schema_path}: {e}")
            # å¯ä»¥åœ¨æ­¤å®šä¹‰å…œåº•å­—å…¸ï¼Œæˆ–è€…ä¾èµ– Formatter çš„ defaults

        try:
            # 1. èåˆä¸å‡†å¤‡æ–‡ä»¶
            gcs_uri, total_scenes = self._prepare_rag_documents(
                dataset=dataset_obj,
                enhanced_facts_path=facts_path,
                staging_dir=staging_dir,
                gcs_bucket_name=gcs_bucket_name,
                org_id=org_id,
                asset_id=asset_id,
                labels=i18n_labels
            )

            # 2. ä¸Šä¼  GCS
            self._upload_dir_to_gcs(staging_dir, gcs_uri)

            # 3. éƒ¨ç½²åˆ° Vertex AI
            self._deploy_to_rag_engine(corpus_display_name, gcs_uri)

            self.logger.info(f"âœ… RAG éƒ¨ç½²æˆåŠŸå®Œæˆã€‚")
            return {
                "message": "RAG deployment initiated.",
                "corpus_name": corpus_display_name,
                "source_gcs_uri": gcs_uri,
                "total_scene_count": total_scenes
            }

        except Exception as e:
            if isinstance(e, (google_exceptions.TooManyRequests, google_exceptions.ResourceExhausted)):
                raise RateLimitException(msg=str(e), provider="GoogleVertexAI") from e
            self.logger.critical(f"éƒ¨ç½²æµç¨‹å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            raise

    def _prepare_rag_documents(self, dataset: NarrativeDataset, enhanced_facts_path: Path,
                               staging_dir: Path, gcs_bucket_name: str,
                               org_id: str, asset_id: str,
                               labels: dict) -> tuple[str, int]:

        self.logger.info("â–¶ï¸ æ­¥éª¤ 1/4: åŠ è½½ Facts å¹¶ä¸ Dataset èåˆ...")

        # A. åŠ è½½ Facts (ä½¿ç”¨ Schema æ ¡éªŒ)
        try:
            with enhanced_facts_path.open('r', encoding='utf-8') as f:
                facts_raw = json.load(f)
            # [Validation] ç¡®ä¿ Facts æ–‡ä»¶æ ¼å¼æ­£ç¡®
            facts_file = CharacterFactsFile(**facts_raw)
        except Exception as e:
            raise ValueError(f"Failed to load facts file: {e}")

        # B. æ•´ç† Facts (æŒ‰ character å½’ç±» -> æ‰“æ•£åˆ° scene)
        # Map: { scene_id(int): [IdentifiedFact, ...] }
        facts_by_scene = defaultdict(list)
        count_facts = 0

        # facts_file.identified_facts_by_character æ˜¯ä¸€ä¸ª Dict[str, List[Dict]]
        for char_name, facts_list in facts_file.identified_facts_by_character.items():
            for fact_dict in facts_list:
                # æ³¨å…¥å½’å±äºº
                fact_dict['character_name'] = char_name
                try:
                    # ä½¿ç”¨ IdentifiedFact Schema å†æ¬¡æ ¡éªŒå•æ¡æ•°æ®
                    fact_obj = IdentifiedFact(**fact_dict)
                    facts_by_scene[fact_obj.scene_id].append(fact_obj)
                    count_facts += 1
                except Exception as e:
                    self.logger.warning(f"Skipping invalid fact for {char_name}: {e}")

        self.logger.info(f"âœ… æ•°æ®èåˆå®Œæˆã€‚Scenes: {len(dataset.scenes)}, Facts: {count_facts}")

        # C. ç”Ÿæˆå¯Œæ–‡æœ¬ (Formatting)
        staging_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"â–¶ï¸ æ­¥éª¤ 2/4: ç”Ÿæˆ RAG å¯Œæ–‡æœ¬æ–‡æ¡£...")

        for scene in dataset.scenes.values():
            # è·å–è¯¥åœºæ™¯å¯¹åº”çš„ Facts
            scene_facts = facts_by_scene.get(scene.local_id, [])

            # ä½¿ç”¨ Formatter ç”Ÿæˆæ–‡æœ¬
            rich_text = RagContentFormatter.format_scene(
                scene=scene,
                facts=scene_facts,
                asset_id=asset_id,
                labels=labels
            )

            # D. å†™å…¥æ–‡ä»¶
            filename = f"{asset_id}_scene_{scene.local_id}_enhanced.txt"
            (staging_dir / filename).write_text(rich_text, encoding='utf-8')

        gcs_uri = f"gs://{gcs_bucket_name}/rag-engine-source/{org_id}/{asset_id}"
        return gcs_uri, len(dataset.scenes)

    def _upload_dir_to_gcs(self, local_dir: Path, gcs_uri: str):
        # ... (ä¿æŒåŸä»£ç ä¸å˜ï¼Œè¿™æ˜¯é€šç”¨çš„) ...
        parts = gcs_uri.replace("gs://", "").split("/", 1)
        bucket = parts[0]
        prefix = parts[1] if len(parts) > 1 else ""

        self.logger.info(f"â–¶ï¸ æ­¥éª¤ 3/4: ä¸Šä¼ è‡³ GCS: {gcs_uri}")
        upload_directory_to_gcs(local_dir, bucket, prefix)

    def _deploy_to_rag_engine(self, corpus_name: str, gcs_uri: str):
        # ... (ä¿æŒåŸä»£ç ä¸å˜) ...
        self.logger.info(f"â–¶ï¸ æ­¥éª¤ 4/4: RAG Engine åŒæ­¥...")
        corpus = self.corpus_manager.get_corpus_by_display_name(corpus_name)
        if not corpus:
            corpus = self.corpus_manager.create_corpus(corpus_name)

        self.data_manager.import_files(corpus.name, [gcs_uri])